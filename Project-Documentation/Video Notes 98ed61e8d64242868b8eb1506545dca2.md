# Video Notes

<aside>
ðŸ’¡ Stochastic Gradient Descent

</aside>

before going for CEFL - check this page for plain vanialla SGD 

[https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32#:~:text=Stochastic Gradient Descent is a probabilistic approximation of Gradient Descent,gradient for the entire dataset](https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32#:~:text=Stochastic%20Gradient%20Descent%20is%20a%20probabilistic%20approximation%20of%20Gradient%20Descent,gradient%20for%20the%20entire%20dataset).

[https://scikit-learn.org/stable/modules/sgd.html](https://scikit-learn.org/stable/modules/sgd.html)

<aside>
ðŸ“Œ **SGD video Summary**

</aside>

![Untitled](Untitled.png)

Can be asynchronous but still server has to wait till the last node completes

Core communication efficient FL

- Below red line is model being pushed to the server rather than the data

![Untitled](Untitled%201.png)

Google and other cos are using the above setup

![Untitled](Untitled%202.png)

clients will perform their own SGD in local and 

![Untitled](Untitled%203.png)

![Untitled](Untitled%204.png)

![Untitled](Untitled%205.png)

![Untitled](Untitled%206.png)

![Untitled](Untitled%207.png)

large T - faster convergence, but large T and stuck to reach global minima

small T - smaller convergence

so Adaptive comm

![Untitled](Untitled%208.png)

performance of adaptive T strategy

![Untitled](Untitled%209.png)

Now, what if T varies across clients

intro to hetegenity in computation in FL

![Untitled](Untitled%2010.png)

most of the algos disregard above in point 3

![Untitled](Untitled%2011.png)

![Untitled](Untitled%2012.png)

![Untitled](Untitled%2013.png)

![Untitled](Untitled%2014.png)

![Untitled](Untitled%2015.png)

![Untitled](Untitled%2016.png)

![Untitled](Untitled%2017.png)

![Untitled](Untitled%2018.png)

![Untitled](Untitled%2019.png)

![Untitled](Untitled%2020.png)